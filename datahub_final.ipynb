{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dm-control in c:\\users\\msi\\anaconda3\\envs\\py310\\lib\\site-packages (1.0.19)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in c:\\users\\msi\\anaconda3\\envs\\py310\\lib\\site-packages (from dm-control) (1.4.0)\n",
      "Requirement already satisfied: dm-env in c:\\users\\msi\\anaconda3\\envs\\py310\\lib\\site-packages (from dm-control) (1.6)\n",
      "Requirement already satisfied: dm-tree!=0.1.2 in c:\\users\\msi\\anaconda3\\envs\\py310\\lib\\site-packages (from dm-control) (0.1.8)\n",
      "Requirement already satisfied: glfw in c:\\users\\msi\\anaconda3\\envs\\py310\\lib\\site-packages (from dm-control) (2.7.0)\n",
      "Requirement already satisfied: labmaze in c:\\users\\msi\\anaconda3\\envs\\py310\\lib\\site-packages (from dm-control) (1.0.6)\n",
      "Requirement already satisfied: lxml in c:\\users\\msi\\anaconda3\\envs\\py310\\lib\\site-packages (from dm-control) (5.1.1)\n",
      "Requirement already satisfied: mujoco>=3.1.5 in c:\\users\\msi\\anaconda3\\envs\\py310\\lib\\site-packages (from dm-control) (3.1.5)\n",
      "Requirement already satisfied: numpy>=1.9.0 in c:\\users\\msi\\anaconda3\\envs\\py310\\lib\\site-packages (from dm-control) (1.23.5)\n",
      "Requirement already satisfied: protobuf>=3.19.4 in c:\\users\\msi\\anaconda3\\envs\\py310\\lib\\site-packages (from dm-control) (3.19.6)\n",
      "Requirement already satisfied: pyopengl>=3.1.4 in c:\\users\\msi\\anaconda3\\envs\\py310\\lib\\site-packages (from dm-control) (3.1.7)\n",
      "Requirement already satisfied: pyparsing>=3.0.0 in c:\\users\\msi\\anaconda3\\envs\\py310\\lib\\site-packages (from dm-control) (3.0.9)\n",
      "Requirement already satisfied: requests in c:\\users\\msi\\anaconda3\\envs\\py310\\lib\\site-packages (from dm-control) (2.31.0)\n",
      "Requirement already satisfied: setuptools!=50.0.0 in c:\\users\\msi\\anaconda3\\envs\\py310\\lib\\site-packages (from dm-control) (68.2.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\msi\\anaconda3\\envs\\py310\\lib\\site-packages (from dm-control) (1.12.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\msi\\anaconda3\\envs\\py310\\lib\\site-packages (from dm-control) (4.65.0)\n",
      "Requirement already satisfied: etils[epath] in c:\\users\\msi\\anaconda3\\envs\\py310\\lib\\site-packages (from mujoco>=3.1.5->dm-control) (1.7.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\msi\\appdata\\roaming\\python\\python310\\site-packages (from requests->dm-control) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\msi\\anaconda3\\envs\\py310\\lib\\site-packages (from requests->dm-control) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\msi\\anaconda3\\envs\\py310\\lib\\site-packages (from requests->dm-control) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\msi\\anaconda3\\envs\\py310\\lib\\site-packages (from requests->dm-control) (2024.2.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\msi\\appdata\\roaming\\python\\python310\\site-packages (from tqdm->dm-control) (0.4.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\msi\\anaconda3\\envs\\py310\\lib\\site-packages (from etils[epath]->mujoco>=3.1.5->dm-control) (2024.3.1)\n",
      "Requirement already satisfied: importlib_resources in c:\\users\\msi\\anaconda3\\envs\\py310\\lib\\site-packages (from etils[epath]->mujoco>=3.1.5->dm-control) (6.4.0)\n",
      "Requirement already satisfied: typing_extensions in c:\\users\\msi\\anaconda3\\envs\\py310\\lib\\site-packages (from etils[epath]->mujoco>=3.1.5->dm-control) (4.10.0)\n",
      "Requirement already satisfied: zipp in c:\\users\\msi\\anaconda3\\envs\\py310\\lib\\site-packages (from etils[epath]->mujoco>=3.1.5->dm-control) (3.18.1)\n"
     ]
    }
   ],
   "source": [
    "! pip install dm-control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchdiffeq in c:\\users\\msi\\anaconda3\\envs\\py310\\lib\\site-packages (0.2.3)\n",
      "Requirement already satisfied: torch>=1.3.0 in c:\\users\\msi\\anaconda3\\envs\\py310\\lib\\site-packages (from torchdiffeq) (2.2.2)\n",
      "Requirement already satisfied: scipy>=1.4.0 in c:\\users\\msi\\anaconda3\\envs\\py310\\lib\\site-packages (from torchdiffeq) (1.12.0)\n",
      "Requirement already satisfied: numpy<1.29.0,>=1.22.4 in c:\\users\\msi\\anaconda3\\envs\\py310\\lib\\site-packages (from scipy>=1.4.0->torchdiffeq) (1.23.5)\n",
      "Requirement already satisfied: filelock in c:\\users\\msi\\anaconda3\\envs\\py310\\lib\\site-packages (from torch>=1.3.0->torchdiffeq) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\msi\\anaconda3\\envs\\py310\\lib\\site-packages (from torch>=1.3.0->torchdiffeq) (4.10.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\msi\\anaconda3\\envs\\py310\\lib\\site-packages (from torch>=1.3.0->torchdiffeq) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\msi\\anaconda3\\envs\\py310\\lib\\site-packages (from torch>=1.3.0->torchdiffeq) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\msi\\anaconda3\\envs\\py310\\lib\\site-packages (from torch>=1.3.0->torchdiffeq) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\msi\\anaconda3\\envs\\py310\\lib\\site-packages (from torch>=1.3.0->torchdiffeq) (2024.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\msi\\anaconda3\\envs\\py310\\lib\\site-packages (from jinja2->torch>=1.3.0->torchdiffeq) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\msi\\anaconda3\\envs\\py310\\lib\\site-packages (from sympy->torch>=1.3.0->torchdiffeq) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install torchdiffeq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.distributions.normal import Normal\n",
    "import numpy as np\n",
    "import time \n",
    "from random import SystemRandom\n",
    "import os\n",
    "from mujoco_physics import HopperPhysics\n",
    "\n",
    "import lib.utils as utils\n",
    "from lib.create_latent_ode_model import create_LatentODE_model\n",
    "from lib.parse_datasets import parse_datasets\n",
    "from lib.utils import compute_loss_all_batches, get_next_batch, makedirs, get_logger\n",
    "\n",
    "from lib.rnn_baselines import *\n",
    "from lib.ode_rnn import *\n",
    "from lib.create_latent_ode_model import create_LatentODE_model\n",
    "from lib.parse_datasets import parse_datasets\n",
    "from lib.ode_func import ODEFunc, ODEFunc_w_Poisson\n",
    "from lib.diffeq_solver import DiffeqSolver\n",
    "from mujoco_physics import HopperPhysics\n",
    "from lib.latent_ode import LatentODE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make changes to all the input args here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Args(\n",
    "    n=10000,  # Size of the dataset\n",
    "    niters=100,\n",
    "    lr=1e-2,  # Starting learning rate\n",
    "    batch_size=50,\n",
    "    viz=False,  # Show plots while training\n",
    "    save='experiments/',  # Path to save checkpoints\n",
    "    load=None,  # ID of the experiment to load for evaluation. If None, run a new experiment\n",
    "    random_seed=1991,  # Random seed\n",
    "    dataset='hopper',  # Dataset to load\n",
    "    sample_tp=None,  # Number of time points to sub-sample\n",
    "    cut_tp=None,  # Cut out the section of the timeline\n",
    "    quantization=0.1,  # Quantization on the physionet dataset\n",
    "    latent_ode=False,  # Run Latent ODE seq2seq model\n",
    "    z0_encoder='odernn',  # Type of encoder for Latent ODE model\n",
    "    classic_rnn=True,  # Run RNN baseline\n",
    "    rnn_cell=\"expdecay\",  # RNN Cell type #gru, expdecay- gru-d\n",
    "    input_decay=True,  # For RNN: use the input that is the weighted average of empirical mean and previous value\n",
    "    ode_rnn=True,  # Run ODE-RNN baseline\n",
    "    rnn_vae=False,  # Run RNN baseline: seq2seq model with sampling of the h0 and ELBO loss\n",
    "    latents=15,  # Size of the latent state\n",
    "    rec_dims=30,  # Dimensionality of the recognition model\n",
    "    rec_layers=3,  # Number of layers in ODE func in recognition ODE\n",
    "    gen_layers=3,  # Number of layers in ODE func in generative ODE\n",
    "    units=300,  # Number of units per layer in ODE func\n",
    "    gru_units=100,  # Number of units per layer in each of GRU update networks\n",
    "    poisson=False,  # Model poisson-process likelihood for the density of events in addition to reconstruction\n",
    "    classif=False,  # Include binary classification loss\n",
    "    linear_classif=False,  # Use a linear classifier instead of 1-layer NN\n",
    "    extrap=False,  # Set extrapolation mode\n",
    "    timepoints=100,  # Total number of time-points\n",
    "    max_t=5.0,  # Subsample points in the interval [0, args.max_t]\n",
    "    noise_weight=0.01  # Noise amplitude for generated trajectories\n",
    ")\n",
    "file_name = \"run_models\"\n",
    "makedirs(args.save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling dataset of 10000 training examples\n",
      "Time taken for setup: 0.0009984970092773438 seconds\n",
      "Input command: run_models.py --n 10000 --niters 10 --lr 0.01 --batch_size 50 --viz False --save experiments/ --random_seed 1991 --dataset hopper --latent_ode False --classic_rnn True --ode_rnn True--z0_encoder odernn --latents 15 --rec_dims 30 --rec_layers 3 --gen_layers 3 --units 300 --gru_units 100 --timepoints 100 --max_t 5.0 --noise_weight 0.01 --extrap False \n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(args.random_seed)\n",
    "np.random.seed(args.random_seed)\n",
    "\n",
    "experimentID = args.load\n",
    "if experimentID is None:\n",
    "\t# Make a new experiment ID\n",
    "\texperimentID = int(SystemRandom().random()*100000)\n",
    "\n",
    "start = time.time()\n",
    "print(\"Sampling dataset of {} training examples\".format(args.n))\n",
    "\n",
    "input_command = f\"run_models.py --n {args.n} --niters {args.niters} --lr {args.lr} --batch_size {args.batch_size} \" \\\n",
    "                f\"--viz {args.viz} --save {args.save} --random_seed {args.random_seed} --dataset {args.dataset} \" \\\n",
    "                f\"--latent_ode {args.latent_ode} --classic_rnn {args.classic_rnn} --ode_rnn {args.ode_rnn}--z0_encoder {args.z0_encoder} --latents {args.latents} \" \\\n",
    "                f\"--rec_dims {args.rec_dims} --rec_layers {args.rec_layers} --gen_layers {args.gen_layers} \" \\\n",
    "                f\"--units {args.units} --gru_units {args.gru_units} --timepoints {args.timepoints} --max_t {args.max_t} \" \\\n",
    "                f\"--noise_weight {args.noise_weight} --extrap {args.extrap} \"\n",
    "\n",
    "if args.load:\n",
    "\tinput_command += f\" --load {args.load}\"\n",
    "\n",
    "makedirs(\"results/\")\n",
    "    \n",
    "print(f\"Time taken for setup: {time.time() - start} seconds\")\n",
    "print(f\"Input command: {input_command}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dimension: 14\n"
     ]
    }
   ],
   "source": [
    "data_obj = parse_datasets(args, device)\n",
    "input_dim = data_obj[\"input_dim\"]\n",
    "\t\n",
    "print(f\"Input dimension: {input_dim}\")\n",
    "\n",
    "classif_per_tp = False\n",
    "if (\"classif_per_tp\" in data_obj):\n",
    "\t\t# do classification per time point rather than on a time series as a whole\n",
    "\t\tclassif_per_tp = data_obj[\"classif_per_tp\"]\n",
    "\n",
    "if args.classif and (args.dataset == \"hopper\" or args.dataset == \"periodic\"):\n",
    "\t\traise Exception(\"Classification task is not available for MuJoCo and 1d datasets\")\n",
    "\n",
    "n_labels = 1\n",
    "if args.classif:\n",
    "\tif (\"n_labels\" in data_obj):\n",
    "\t\tn_labels = data_obj[\"n_labels\"]\n",
    "\telse:\n",
    "\t\traise Exception(\"Please provide number of labels for classification task\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "obsrv_std = 1e-3 \n",
    "obsrv_std = torch.Tensor([obsrv_std]).to(device)\n",
    "z0_prior = Normal(torch.Tensor([0.0]).to(device), torch.Tensor([1.]).to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.rnn_vae:\n",
    "\t\tif args.poisson:\n",
    "\t\t\tprint(\"Poisson process likelihood not implemented for RNN-VAE: ignoring --poisson\")\n",
    "\n",
    "\t\t# Create RNN-VAE model\n",
    "\t\tmodel = RNN_VAE(input_dim, args.latents, \n",
    "\t\t\tdevice = device, \n",
    "\t\t\trec_dims = args.rec_dims, \n",
    "\t\t\tconcat_mask = True, \n",
    "\t\t\tobsrv_std = obsrv_std,\n",
    "\t\t\tz0_prior = z0_prior,\n",
    "\t\t\tuse_binary_classif = args.classif,\n",
    "\t\t\tclassif_per_tp = classif_per_tp,\n",
    "\t\t\tlinear_classifier = args.linear_classif,\n",
    "\t\t\tn_units = args.units,\n",
    "\t\t\tinput_space_decay = args.input_decay,\n",
    "\t\t\tcell = args.rnn_cell,\n",
    "\t\t\tn_labels = n_labels,\n",
    "\t\t\ttrain_classif_w_reconstr = (args.dataset == \"physionet\")\n",
    "\t\t\t).to(device)\n",
    "elif args.classic_rnn:\n",
    "\t\tif args.poisson:\n",
    "\t\t\tprint(\"Poisson process likelihood not implemented for RNN: ignoring --poisson\")\n",
    "\n",
    "\t\tif args.extrap:\n",
    "\t\t\traise Exception(\"Extrapolation for standard RNN not implemented\")\n",
    "\t\t# Create RNN model\n",
    "\t\tmodel = Classic_RNN(input_dim, args.latents, device, \n",
    "\t\t\tconcat_mask = True, obsrv_std = obsrv_std,\n",
    "\t\t\tn_units = args.units,\n",
    "\t\t\tuse_binary_classif = args.classif,\n",
    "\t\t\tclassif_per_tp = classif_per_tp,\n",
    "\t\t\tlinear_classifier = args.linear_classif,\n",
    "\t\t\tinput_space_decay = args.input_decay,\n",
    "\t\t\tcell = args.rnn_cell,\n",
    "\t\t\tn_labels = n_labels,\n",
    "\t\t\ttrain_classif_w_reconstr = (args.dataset == \"physionet\")\n",
    "\t\t\t).to(device)\n",
    "elif args.ode_rnn:\n",
    "\t\t# Create ODE-GRU model\n",
    "\t\tn_ode_gru_dims = args.latents\n",
    "\t\t\t\t\n",
    "\t\tif args.poisson:\n",
    "\t\t\tprint(\"Poisson process likelihood not implemented for ODE-RNN: ignoring --poisson\")\n",
    "\n",
    "\t\tif args.extrap:\n",
    "\t\t\traise Exception(\"Extrapolation for ODE-RNN not implemented\")\n",
    "\n",
    "\t\tode_func_net = utils.create_net(n_ode_gru_dims, n_ode_gru_dims, \n",
    "\t\t\tn_layers = args.rec_layers, n_units = args.units, nonlinear = nn.Tanh)\n",
    "\n",
    "\t\trec_ode_func = ODEFunc(\n",
    "\t\t\tinput_dim = input_dim, \n",
    "\t\t\tlatent_dim = n_ode_gru_dims,\n",
    "\t\t\tode_func_net = ode_func_net,\n",
    "\t\t\tdevice = device).to(device)\n",
    "\n",
    "\t\tz0_diffeq_solver = DiffeqSolver(input_dim, rec_ode_func, \"euler\", args.latents, \n",
    "\t\t\todeint_rtol = 1e-3, odeint_atol = 1e-4, device = device)\n",
    "\t\n",
    "\t\tmodel = ODE_RNN(input_dim, n_ode_gru_dims, device = device, \n",
    "\t\t\tz0_diffeq_solver = z0_diffeq_solver, n_gru_units = args.gru_units,\n",
    "\t\t\tconcat_mask = True, obsrv_std = obsrv_std,\n",
    "\t\t\tuse_binary_classif = args.classif,\n",
    "\t\t\tclassif_per_tp = classif_per_tp,\n",
    "\t\t\tn_labels = n_labels,\n",
    "\t\t\ttrain_classif_w_reconstr = (args.dataset == \"physionet\")\n",
    "\t\t\t).to(device)\n",
    "elif args.latent_ode:\n",
    "\t\tmodel = create_LatentODE_model(args, input_dim, z0_prior, obsrv_std, device, \n",
    "\t\t\tclassif_per_tp = classif_per_tp,\n",
    "\t\t\tn_labels = n_labels)\n",
    "else:\n",
    "\traise Exception(\"Model not specified\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiments/experiment_Classic_RNN_None.ckpt\n"
     ]
    }
   ],
   "source": [
    "ckpt_path = os.path.join(args.save, \"experiment_\" + model.__class__.__name__+\"_\"+ str(args.sample_tp)+ '.ckpt')\n",
    "print(ckpt_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the path according to your environment (script_path variable) (Strictly run this only once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\msi\\Desktop\\ECE-228\\Project\\latent_ode_ece_228\n",
      "run_models.py --n 10000 --niters 10 --lr 0.01 --batch_size 50 --viz False --save experiments/ --random_seed 1991 --dataset hopper --latent_ode False --classic_rnn True --ode_rnn True--z0_encoder odernn --latents 15 --rec_dims 30 --rec_layers 3 --gen_layers 3 --units 300 --gru_units 100 --timepoints 100 --max_t 5.0 --noise_weight 0.01 --extrap False \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logs/run_models_Classic_RNN_None.log\n"
     ]
    }
   ],
   "source": [
    "log_path = \"logs/\" + file_name + \"_\" + model.__class__.__name__+\"_\" +str(args.sample_tp)+ \".log\"\n",
    "print(log_path)\n",
    "if not os.path.exists(\"logs/\"):\n",
    "\tutils.makedirs(\"logs/\")\n",
    "script_path = os.path.abspath(r\"C:\\Users\\msi\\Desktop\\ECE-228\\Project\\latent_ode_ece_228\") \n",
    "\n",
    "logger = get_logger(logpath=log_path, filepath=script_path)\n",
    "logger.info(input_command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adamax(model.parameters(), lr=args.lr)\n",
    "num_batches = data_obj[\"n_train_batches\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing loss... 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Experiment 69120\n",
      "Epoch 0001 [Test seq (cond on sampled tp)] | Loss 9769.706055 | Likelihood -9769.706055 | KL fp 0.0000 | FP STD 0.0000|\n",
      "KL coef: 0.0\n",
      "Train loss (one batch): 9318.19140625\n",
      "Train CE loss (one batch): 0.0\n",
      "Test MSE: 0.0196\n",
      "Poisson likelihood: 0.0\n",
      "CE loss: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing loss... 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Experiment 69120\n",
      "Epoch 0002 [Test seq (cond on sampled tp)] | Loss 5388.971191 | Likelihood -5388.971191 | KL fp 0.0000 | FP STD 0.0000|\n",
      "KL coef: 0.0\n",
      "Train loss (one batch): 5047.59423828125\n",
      "Train CE loss (one batch): 0.0\n",
      "Test MSE: 0.0108\n",
      "Poisson likelihood: 0.0\n",
      "CE loss: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing loss... 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Experiment 69120\n",
      "Epoch 0003 [Test seq (cond on sampled tp)] | Loss 3402.090332 | Likelihood -3402.090332 | KL fp 0.0000 | FP STD 0.0000|\n",
      "KL coef: 0.0\n",
      "Train loss (one batch): 3004.2705078125\n",
      "Train CE loss (one batch): 0.0\n",
      "Test MSE: 0.0068\n",
      "Poisson likelihood: 0.0\n",
      "CE loss: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing loss... 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Experiment 69120\n",
      "Epoch 0004 [Test seq (cond on sampled tp)] | Loss 2732.810791 | Likelihood -2732.810791 | KL fp 0.0000 | FP STD 0.0000|\n",
      "KL coef: 0.0\n",
      "Train loss (one batch): 2156.554443359375\n",
      "Train CE loss (one batch): 0.0\n",
      "Test MSE: 0.0055\n",
      "Poisson likelihood: 0.0\n",
      "CE loss: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing loss... 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Experiment 69120\n",
      "Epoch 0005 [Test seq (cond on sampled tp)] | Loss 1869.080444 | Likelihood -1869.080444 | KL fp 0.0000 | FP STD 0.0000|\n",
      "KL coef: 0.0\n",
      "Train loss (one batch): 1651.75390625\n",
      "Train CE loss (one batch): 0.0\n",
      "Test MSE: 0.0038\n",
      "Poisson likelihood: 0.0\n",
      "CE loss: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing loss... 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Experiment 69120\n",
      "Epoch 0006 [Test seq (cond on sampled tp)] | Loss 1688.994141 | Likelihood -1688.994141 | KL fp 0.0000 | FP STD 0.0000|\n",
      "KL coef: 0.0\n",
      "Train loss (one batch): 1570.232177734375\n",
      "Train CE loss (one batch): 0.0\n",
      "Test MSE: 0.0034\n",
      "Poisson likelihood: 0.0\n",
      "CE loss: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing loss... 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Experiment 69120\n",
      "Epoch 0007 [Test seq (cond on sampled tp)] | Loss 1326.463257 | Likelihood -1326.463257 | KL fp 0.0000 | FP STD 0.0000|\n",
      "KL coef: 0.0\n",
      "Train loss (one batch): 1331.3428955078125\n",
      "Train CE loss (one batch): 0.0\n",
      "Test MSE: 0.0027\n",
      "Poisson likelihood: 0.0\n",
      "CE loss: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing loss... 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Experiment 69120\n",
      "Epoch 0008 [Test seq (cond on sampled tp)] | Loss 1113.009277 | Likelihood -1113.009277 | KL fp 0.0000 | FP STD 0.0000|\n",
      "KL coef: 0.0\n",
      "Train loss (one batch): 1212.82373046875\n",
      "Train CE loss (one batch): 0.0\n",
      "Test MSE: 0.0022\n",
      "Poisson likelihood: 0.0\n",
      "CE loss: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing loss... 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Experiment 69120\n",
      "Epoch 0009 [Test seq (cond on sampled tp)] | Loss 989.525452 | Likelihood -989.525452 | KL fp 0.0000 | FP STD 0.0000|\n",
      "KL coef: 0.0\n",
      "Train loss (one batch): 1018.4262084960938\n",
      "Train CE loss (one batch): 0.0\n",
      "Test MSE: 0.0020\n",
      "Poisson likelihood: 0.0\n",
      "CE loss: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing loss... 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Experiment 69120\n",
      "Epoch 0010 [Test seq (cond on sampled tp)] | Loss 821.759766 | Likelihood -821.759766 | KL fp 0.0000 | FP STD 0.0000|\n",
      "KL coef: 0.0\n",
      "Train loss (one batch): 874.3020629882812\n",
      "Train CE loss (one batch): 0.0\n",
      "Test MSE: 0.0017\n",
      "Poisson likelihood: 0.0\n",
      "CE loss: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete. Model saved.\n"
     ]
    }
   ],
   "source": [
    "for itr in range(1, num_batches * (args.niters + 1)):\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\tutils.update_learning_rate(optimizer, decay_rate = 0.999, lowest = args.lr / 10)\n",
    "\n",
    "\t\twait_until_kl_inc = 10\n",
    "\t\tif itr // num_batches < wait_until_kl_inc:\n",
    "\t\t\tkl_coef = 0.\n",
    "\t\telse:\n",
    "\t\t\tkl_coef = (1-0.99** (itr // num_batches - wait_until_kl_inc))\n",
    "\n",
    "\t\tbatch_dict = utils.get_next_batch(data_obj[\"train_dataloader\"])\n",
    "\n",
    "\t\ttrain_res = model.compute_all_losses(batch_dict, n_traj_samples = 3, kl_coef = kl_coef)\n",
    "\t\ttrain_res[\"loss\"].backward()\n",
    "\t\toptimizer.step()\n",
    "\n",
    "\t\tn_iters_to_viz = 1\n",
    "\t\tif itr % (n_iters_to_viz * num_batches) == 0:\n",
    "\t\t\twith torch.no_grad():\n",
    "\n",
    "\t\t\t\ttest_res = compute_loss_all_batches(model, \n",
    "\t\t\t\t\tdata_obj[\"test_dataloader\"], args,\n",
    "\t\t\t\t\tn_batches = data_obj[\"n_test_batches\"],\n",
    "\t\t\t\t\texperimentID = experimentID,\n",
    "\t\t\t\t\tdevice = device,\n",
    "\t\t\t\t\tn_traj_samples = 3, kl_coef = kl_coef)\n",
    "\n",
    "\t\t\t\tmessage = 'Epoch {:04d} [Test seq (cond on sampled tp)] | Loss {:.6f} | Likelihood {:.6f} | KL fp {:.4f} | FP STD {:.4f}|'.format(\n",
    "\t\t\t\t\titr//num_batches, \n",
    "\t\t\t\t\ttest_res[\"loss\"].detach(), test_res[\"likelihood\"].detach(), \n",
    "\t\t\t\t\ttest_res[\"kl_first_p\"], test_res[\"std_first_p\"])\n",
    "\t\t \t\n",
    "\t\t\t\tlogger.info(\"Experiment \" + str(experimentID))\n",
    "\t\t\t\tlogger.info(message)\n",
    "\t\t\t\tlogger.info(\"KL coef: {}\".format(kl_coef))\n",
    "\t\t\t\tlogger.info(\"Train loss (one batch): {}\".format(train_res[\"loss\"].detach()))\n",
    "\t\t\t\tlogger.info(\"Train CE loss (one batch): {}\".format(train_res[\"ce_loss\"].detach()))\n",
    "\t\t\t\t\n",
    "\t\t\t\tif \"auc\" in test_res:\n",
    "\t\t\t\t\tlogger.info(\"Classification AUC (TEST): {:.4f}\".format(test_res[\"auc\"]))\n",
    "\n",
    "\t\t\t\tif \"mse\" in test_res:\n",
    "\t\t\t\t\tlogger.info(\"Test MSE: {:.4f}\".format(test_res[\"mse\"]))\n",
    "\n",
    "\t\t\t\tif \"accuracy\" in train_res:\n",
    "\t\t\t\t\tlogger.info(\"Classification accuracy (TRAIN): {:.4f}\".format(train_res[\"accuracy\"]))\n",
    "\n",
    "\t\t\t\tif \"accuracy\" in test_res:\n",
    "\t\t\t\t\tlogger.info(\"Classification accuracy (TEST): {:.4f}\".format(test_res[\"accuracy\"]))\n",
    "\n",
    "\t\t\t\tif \"pois_likelihood\" in test_res:\n",
    "\t\t\t\t\tlogger.info(\"Poisson likelihood: {}\".format(test_res[\"pois_likelihood\"]))\n",
    "\n",
    "\t\t\t\tif \"ce_loss\" in test_res:\n",
    "\t\t\t\t\tlogger.info(\"CE loss: {}\".format(test_res[\"ce_loss\"]))\n",
    "\n",
    "\t\t\ttorch.save({\n",
    "\t\t\t\t'args': args,\n",
    "\t\t\t\t'state_dict': model.state_dict(),\n",
    "\t\t\t}, ckpt_path)\n",
    "torch.save({\n",
    "    'args': args,\n",
    "    'state_dict': model.state_dict(),\n",
    "}, ckpt_path)\n",
    "\n",
    "print(\"Training complete. Model saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path=r\"C:\\Users\\msi\\Desktop\\ECE-228\\Project\\latent_ode_ece_228\\experiments\\experiment_Classic_RNN_None.ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.get_ckpt_model(ckpt_path, model, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
